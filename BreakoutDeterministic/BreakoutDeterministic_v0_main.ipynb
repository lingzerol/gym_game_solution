{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0oTjL0cHEuHL",
    "outputId": "3abf30a0-7ba3-4051-e24f-e49c869df315"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "from gym.core import Wrapper\n",
    "from gym.core import ObservationWrapper\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D,Dense,Flatten\n",
    "import keras\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "import dill\n",
    "import getopt,sys\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import cv2\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "a=[2]\n",
    "a.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utQEzQCyAhIg"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FrameBuffer(Wrapper):\n",
    "    def __init__(self, env, n_frames=4, dim_order='tensorflow'):\n",
    "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
    "        super(FrameBuffer, self).__init__(env)\n",
    "        self.dim_order = dim_order\n",
    "        if dim_order == 'tensorflow':\n",
    "            height, width, n_channels = env.observation_space.shape\n",
    "            obs_shape = [height, width, n_channels * n_frames]\n",
    "        elif dim_order == 'pytorch':\n",
    "            n_channels, height, width = env.observation_space.shape\n",
    "            obs_shape = [n_channels * n_frames, height, width]\n",
    "        else:\n",
    "            raise ValueError('dim_order should be \"tensorflow\" or \"pytorch\", got {}'.format(dim_order))\n",
    "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
    "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
    "        self.update_buffer(self.env.reset())\n",
    "        return self.framebuffer\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
    "        new_img, reward, done, info = self.env.step(action)\n",
    "        self.update_buffer(new_img)\n",
    "        return self.framebuffer, reward, done, info\n",
    "    \n",
    "    def update_buffer(self, img):\n",
    "        if self.dim_order == 'tensorflow':\n",
    "            offset = self.env.observation_space.shape[-1]\n",
    "            axis = -1\n",
    "            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
    "        elif self.dim_order == 'pytorch':\n",
    "            offset = self.env.observation_space.shape[0]\n",
    "            axis = 0\n",
    "            cropped_framebuffer = self.framebuffer[:-offset]\n",
    "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n",
    "\n",
    "\n",
    "\n",
    "# This code is shamelessly stolen from https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "\n",
    "class PreprocessAtari(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it.\"\"\"\n",
    "        ObservationWrapper.__init__(self,env)\n",
    "        \n",
    "        self.img_size = (64, 64)\n",
    "        self.observation_space = Box(0.0, 1.0, (self.img_size[0], self.img_size[1], 1))\n",
    "\n",
    "    def observation(self, img):\n",
    "\n",
    "        img = img[34:-16, :, :]\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        img = img.mean(-1, keepdims=True)               \n",
    "        img = img.astype('float32') / 255.\n",
    "               \n",
    "        return img\n",
    "\n",
    "\n",
    "def makeAtarienv(game:str,n_frames=4):\n",
    "    env=gym.make(game)\n",
    "    env = PreprocessAtari(env)\n",
    "    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
    "    return env\n",
    "\n",
    "class DQNNetwork:\n",
    "    \"\"\"\n",
    "    class implement deep q learning\n",
    "    \"\"\"\n",
    "    def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            \n",
    "            self.network = keras.models.Sequential()\n",
    "            self.network.add(Conv2D(16, (3, 3), strides=2, activation='relu', input_shape=state_shape))\n",
    "            self.network.add(Conv2D(32, (3, 3), strides=2, activation='relu'))\n",
    "            self.network.add(Conv2D(64, (3, 3), strides=2, activation='relu'))\n",
    "            self.network.add(Conv2D(128, (3, 3), strides=2, activation='relu'))\n",
    "            self.network.add(Flatten())\n",
    "            self.network.add(Dense(512, activation='relu'))\n",
    "            self.network.add(Dense(256, activation='relu'))\n",
    "            self.network.add(Dense(128, activation='relu'))\n",
    "            self.network.add(Dense(n_actions, activation='linear'))\n",
    "            \n",
    "            # prepare a graph for agent step\n",
    "            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n",
    "            self.qvalues_t = self.get_symbolic_qvalues(self.state_t)\n",
    "            \n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    def get_symbolic_qvalues(self, state_t):\n",
    "        \"\"\"takes agent's observation, returns qvalues. Both are tf Tensors\"\"\"\n",
    "        \n",
    "        qvalues = self.network(state_t)        \n",
    "        return qvalues\n",
    "    \n",
    "    def get_qvalues(self, state_t):\n",
    "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.qvalues_t, {self.state_t: state_t})\n",
    "    \n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,env,epsilon=0,gamma=0.99,decrease_epsilon=0.99,load_sess=False,file=\"\",GPU_enable=False,global_step=1,reuse=False):\n",
    "\n",
    "        self.env=env\n",
    "        print(env)\n",
    "        self.state_shape=env.observation_space.shape\n",
    "        self.n_actions=self.env.action_space.n\n",
    "        self.epsilon=epsilon\n",
    "        self.exp_replay=ReplayBuffer(50000)\n",
    "        self.latest_exp_reply=ReplayBuffer(300)\n",
    "        if GPU_enable:\n",
    "            device=\"/device:GPU:0\"\n",
    "        else:\n",
    "            device=\"/cpu:0\"\n",
    "        \n",
    "        with tf.device(device):\n",
    "          \n",
    "        #with tf.device(\"/device:GPU:0\"):\n",
    "            #with tf.variable_scope(\"DQNAgent\",reuse=reuse):\n",
    "            self.network = DQNNetwork(\"network\", self.state_shape, self.n_actions,self.epsilon);\n",
    "            self.target_network = DQNNetwork(\"target_network\", self.state_shape, self.n_actions,self.epsilon);\n",
    "            self.obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim,name=\"state\")\n",
    "            self.actions_ph = tf.placeholder(tf.int32, shape=[None],name=\"actions\")\n",
    "            self.rewards_ph = tf.placeholder(tf.float32, shape=[None],name=\"rewards\")\n",
    "            self.next_obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim,name=\"next_ss\")\n",
    "            self.is_done_ph = tf.placeholder(tf.float32, shape=[None],name=\"done\")\n",
    "\n",
    "            is_not_done = 1 - self.is_done_ph\n",
    "            self.gamma=gamma\n",
    "\n",
    "            current_qvalues = self.network.get_symbolic_qvalues(self.obs_ph)\n",
    "            current_action_qvalues = tf.reduce_sum(tf.one_hot(self.actions_ph, n_actions) * current_qvalues, axis=1)\n",
    "\n",
    "            next_qvalues_target = self.target_network.get_symbolic_qvalues(self.next_obs_ph)\n",
    "\n",
    "            next_state_values_target = tf.reduce_max(next_qvalues_target, axis=-1)\n",
    "\n",
    "            # compute Q_reference(s,a) as per formula above.\n",
    "            reference_qvalues = self.rewards_ph + self.gamma*next_state_values_target*is_not_done\n",
    "\n",
    "            # Define loss function for sgd.\n",
    "            td_loss = (current_action_qvalues - reference_qvalues) ** 2\n",
    "            self.td_loss = tf.reduce_mean(td_loss)\n",
    "\n",
    "            self.train_step = tf.train.AdamOptimizer(1e-3).minimize(self.td_loss, var_list=self.network.weights)\n",
    "        sess = tf.get_default_session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if load_sess==True:\n",
    "            self.load(file,global_step)\n",
    "\n",
    "    def load(self,file:str,global_step=1):\n",
    "        sess = tf.get_default_session()\n",
    "        saver=tf.train.Saver()\n",
    "        saver.restore(sess,file+\"-\"+str(global_step))\n",
    "    def save(self,file:str,global_step=1):\n",
    "        sess = tf.get_default_session()\n",
    "        saver=tf.train.Saver()\n",
    "        saver.save(sess,file,global_step=global_step)\n",
    "\n",
    "        \n",
    "    def evaluate(self,n_games=1, greedy=False, t_max=10000):\n",
    "      rewards = []\n",
    "      for _ in range(n_games):\n",
    "          s = self.env.reset()\n",
    "          reward = 0\n",
    "          for _ in range(t_max):\n",
    "              qvalues = self.network.get_qvalues([s])\n",
    "              action = qvalues.argmax(axis=-1)[0] if greedy else self.network.sample_actions(qvalues)[0]\n",
    "              s, r, done, _ = self.env.step(action)\n",
    "              reward += r\n",
    "              if done: break\n",
    "\n",
    "          rewards.append(reward)\n",
    "      return np.mean(rewards)\n",
    "\n",
    "    def play_and_record(self, n_steps=1):\n",
    "        s = self.env.framebuffer\n",
    "        reward = 0.0\n",
    "        for t in range(n_steps):\n",
    "            # get agent to pick action given state s\n",
    "            qvalues = self.network.get_qvalues([s])\n",
    "            action = self.network.sample_actions(qvalues)[0]\n",
    "            next_s, r, done, _ = self.env.step(action)\n",
    "\n",
    "            # add to replay buffer\n",
    "            self.exp_replay.add(s, action, r, next_s, done)\n",
    "            self.latest_exp_reply.add(s,action,r,next_s,done)\n",
    "            reward += r\n",
    "            if done:\n",
    "                s = self.env.reset()\n",
    "            else:\n",
    "                s = next_s\n",
    "        #sess.run(self.train_step, self.sample_batch(batch_size=64,latest=True))       \n",
    "        return reward\n",
    "    \n",
    "    def load_weigths_into_target_network(self):\n",
    "        \"\"\" assign target_network.weights variables to their respective agent.weights values. \"\"\"\n",
    "        assigns = []\n",
    "        for w_agent, w_target in zip(self.network.weights, self.target_network.weights):\n",
    "            assigns.append(tf.assign(w_target, w_agent, validate_shape=True))\n",
    "        tf.get_default_session().run(assigns)\n",
    "      \n",
    "    def sample_batch(self, batch_size,latest=False):\n",
    "        if latest==False:\n",
    "            obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = self.exp_replay.sample(batch_size)\n",
    "        else:\n",
    "            obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = self.latest_exp_reply.sample(batch_size)\n",
    "        return {\n",
    "          self.obs_ph:obs_batch, self.actions_ph:act_batch, self.rewards_ph:reward_batch, \n",
    "          self.next_obs_ph:next_obs_batch, self.is_done_ph:is_done_batch\n",
    "      }\n",
    "    \n",
    "    def fit(self,t_max=10**5):\n",
    "        mean_rw_history = []\n",
    "        td_loss_history = []\n",
    "        #self.play_and_record(10000)\n",
    "        moving_average = lambda x, span, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(span=span, **kw).mean().values\n",
    "        sess=tf.get_default_session()\n",
    "        for i in trange(t_max):  \n",
    "            # play\n",
    "            self.play_and_record(600)\n",
    "\n",
    "            # train\n",
    "            _, loss_t = sess.run([self.train_step, self.td_loss], self.sample_batch(batch_size=64))\n",
    "            td_loss_history.append(loss_t)\n",
    "\n",
    "            # adjust agent parameters\n",
    "            if i % 500 == 0:\n",
    "                self.load_weigths_into_target_network()\n",
    "                self.network.epsilon = max(self.network.epsilon * 0.99, 0.01)\n",
    "                mean_rw_history.append(self.evaluate(n_games=3))\n",
    "\n",
    "                if np.mean(mean_rw_history[-10:]) > 60:\n",
    "                    print('Should be ok')\n",
    "                    break\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                clear_output(True)\n",
    "                print(\"buffer size = %i, epsilon = %.5f\" % (len(self.exp_replay), self.network.epsilon))\n",
    "\n",
    "                plt.figure(figsize=[48, 4])\n",
    "                plt.subplot(1,2,1)\n",
    "                plt.title(\"mean reward per game\")\n",
    "                plt.plot(mean_rw_history)\n",
    "                plt.grid()\n",
    "\n",
    "                assert not np.isnan(loss_t)\n",
    "                plt.figure(figsize=[48, 4])\n",
    "                plt.subplot(1,2,2)\n",
    "                plt.title(\"TD loss history (moving average)\")\n",
    "                plt.plot(moving_average(np.array(td_loss_history), span=100, min_periods=100))\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "                if(len(td_loss_history)>10000):\n",
    "                    td_loss_history.clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show(mean_rw_history,td_loss_history):\n",
    "    moving_average = lambda x, span, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(span=span, **kw).mean().values\n",
    "    plt.figure(figsize=[12,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"mean reward per game\")\n",
    "    plt.plot(mean_rw_history)\n",
    "    plt.grid()\n",
    "\n",
    "    #plt.figure(figsize=[12, 4])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"TD loss history (moving average)\")\n",
    "    plt.plot(moving_average(td_loss_history, span=100, min_periods=100))\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "r9YPF3u7A0DG",
    "outputId": "8a05b145-a568-43f2-8649-4355a9350b7a"
   },
   "outputs": [],
   "source": [
    "env=makeAtarienv(\"BreakoutDeterministic-v4\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "agent=DQNAgent(env,epsilon=0.5,GPU_enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsD9Ej1tVIaM"
   },
   "outputs": [],
   "source": [
    "agent.network.epsilon=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "g6WEg_DPBTfe",
    "outputId": "543e1af2-41d0-46e2-a43c-a134e878c8e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdGW9g34NM3S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuxPen7cBrIX"
   },
   "outputs": [],
   "source": [
    "agent.network.epsilon=0 # Don't forget to reset epsilon back to previous value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rVMgiRwhBrvG"
   },
   "outputs": [],
   "source": [
    "sessions = [agent.evaluate() for _ in range(100)]\n",
    "print(np.array(sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fV_Caj8FRxw"
   },
   "outputs": [],
   "source": [
    "s=env.reset()\n",
    "greedy=True\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "\n",
    "while True:\n",
    "  clear_output(True)\n",
    "  qvalues = agent.network.get_qvalues([s])\n",
    "  action = qvalues.argmax(axis=-1)[0] if greedy else agent.network.sample_actions(qvalues)[0]\n",
    "  new_s,r,done,info=env.step(action)\n",
    "  plt.imshow(env.render('rgb_array'))\n",
    "  plt.pause(0.1)\n",
    "  plt.close()\n",
    "  s=new_s\n",
    "  \n",
    "  if done:\n",
    "    s=env.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foOSawTrBt1m"
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env.reset()\n",
    "env_monitor = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "agent.env=env_monitor\n",
    "sessions = [agent.evaluate(n_games=1) for _ in range(100)]\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nb1nP3FvB9lG"
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "print(video_names[0])\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjfhtrU_OZJX"
   },
   "outputs": [],
   "source": [
    "agent.save(\"./model.ckpt\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "ap70_QbDOZK4",
    "outputId": "799dd86b-7287-4d3b-f519-6501fb0292aa"
   },
   "outputs": [],
   "source": [
    "os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340,
     "resources": {
      "http://localhost:15373/content/sample_data": {
       "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNTAxIChOb3QgSW1wbGVtZW50ZWQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj41MDEuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
       "headers": [
        [
         "content-length",
         "1455"
        ],
        [
         "content-type",
         "text/html; charset=utf-8"
        ]
       ],
       "ok": false,
       "status": 501,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Aqlss5w-OHWX",
    "outputId": "862671fa-a477-40b0-b12a-e896af335263"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('./sample_data') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "T0t99BhceH59",
    "outputId": "832ac3a8-428b-4c43-bed2-47d3d2cfee8c"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.upload()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BreakoutDeterministic_v0_main.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
